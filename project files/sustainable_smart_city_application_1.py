# -*- coding: utf-8 -*-
"""SUSTAINABLE SMART CITY APPLICATION 1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W-1TKLpWnVUbz1D4ZJdjLgIo4XDTDhVr
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install transformers gradio
# %pip install accelerate bitsandbytes scipy

"""## Load the ibm granite model

### Subtask:
Load the specified IBM Granite model using the Hugging Face `transformers` library.

**Reasoning**:
Load the tokenizer and model for the specified IBM Granite model.
"""

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model_name = "ibm-granite/granite-3.3-2b-instruct"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, low_cpu_mem_usage=True)

"""## Develop the gradio application

### Subtask:
Create the main Gradio application structure with different interfaces for each functionality.

**Reasoning**:
Define placeholder functions for each functionality and create a Gradio interface for each, then combine them into a tabbed interface and launch the app.
"""

import gradio as gr

def city_health_dashboard(state, country, place):
    """Placeholder function for City Health Dashboard."""
    return f"City Health Dashboard for {place}, {state}, {country} - Data coming soon!"

def citizen_feedback_system(feedback):
    """Placeholder function for Citizen Feedback System."""
    return f"Received feedback: {feedback}\nSummary and routing coming soon!"

def document_summarization_tool(document):
    """Placeholder function for Document Summarization Tool."""
    if document is None:
        return "Please upload a document."
    return "Document uploaded. Summarization coming soon!"

def eco_advice_assistant(query):
    """Placeholder function for Eco-Advice Assistant."""
    return f"Eco-advice for '{query}' coming soon!"

# Create Gradio Interfaces for each functionality
health_interface = gr.Interface(
    fn=city_health_dashboard,
    inputs=[gr.Textbox(label="State"), gr.Textbox(label="Country"), gr.Textbox(label="Place")],
    outputs="text",
    title="City Health Dashboard"
)

feedback_interface = gr.Interface(
    fn=citizen_feedback_system,
    inputs=gr.Textbox(label="Enter your feedback"),
    outputs="text",
    title="Citizen Feedback System"
)

summarization_interface = gr.Interface(
    fn=document_summarization_tool,
    inputs=gr.File(label="Upload Document (PDF)"),
    outputs="text",
    title="Document Summarization Tool"
)

eco_interface = gr.Interface(
    fn=eco_advice_assistant,
    inputs=gr.Textbox(label="Ask for eco-tips"),
    outputs="text",
    title="Eco-Advice Assistant"
)

# Combine interfaces into a TabbedInterface
app = gr.TabbedInterface(
    [health_interface, feedback_interface, summarization_interface, eco_interface],
    ["City Health Dashboard", "Citizen Feedback System", "Document Summarization Tool", "Eco-Advice Assistant"],
    title="SUSTAINABLE SMART CITY ASSISTANT"
)

# Launch the Gradio application
app.launch(inline=True)

"""## Implement the city health dashboard

### Subtask:
Implement the city health dashboard with input fields for location and placeholder data displayed in a user-friendly format.

"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install PyPDF2

"""**Reasoning**:
Implement the city health dashboard with input fields for location and placeholder data displayed in a user-friendly format within the `city_health_dashboard` function, and update the Gradio Interface accordingly.


"""

import gradio as gr
import PyPDF2
import io # Import io for file handling
import matplotlib.pyplot as plt # Import matplotlib for plotting
import torch # Import torch for GPU handling

# tokenizer and model are assumed to be loaded from a previous cell

def city_health_dashboard(state, country, place):
    """Provides placeholder data for City Health Dashboard based on location."""
    if not state and not country and not place:
        return "Please enter a location (State, Country, or Place) to view the dashboard."

    # Placeholder data - In a real application, this would be replaced with actual data retrieval
    air_quality_data = {"PM2.5": 15, "AQI": 55} # Changed to numerical for plotting
    noise_levels_data = {"Average dB": 60, "Peak dB": 75} # Changed to numerical for plotting
    water_quality_data = {"pH": 7.2, "Turbidity": 5} # Changed to numerical for plotting
    traffic_data = {"Current Speed": 25, "Congestion Level (1-5)": 3} # Changed to numerical for plotting
    energy_usage_data = {"Daily Consumption (kWh/household)": 15, "Renewable Share (%)": 20} # Changed to numerical for plotting

    output_text = f"## City Health Data for {place}, {state}, {country}\n\n"

    # Example of generating a simple plot for Air Quality data
    try:
        fig, ax = plt.subplots()
        categories = list(air_quality_data.keys())
        values = list(air_quality_data.values())
        ax.bar(categories, values)
        ax.set_ylabel("Value")
        ax.set_title("Air Quality Data")
        plt.tight_layout()
        # Save the plot to a file and include it in the markdown output
        plot_path = f"/tmp/air_quality_{place}.png"
        plt.savefig(plot_path)
        plt.close(fig) # Close the plot to free memory
        output_text += f"### Air Quality\n"
        output_text += f"![Air Quality Plot]({plot_path})\n\n"
        for key, value in air_quality_data.items():
             output_text += f"- **{key}:** {value}\n"

    except Exception as e:
        output_text += f"### Air Quality\nError generating plot: {e}\n"
        for key, value in air_quality_data.items():
            output_text += f"- **{key}:** {value}\n"


    output_text += "\n### Noise Levels\n"
    for key, value in noise_levels_data.items():
        output_text += f"- **{key}:** {value}\n"

    output_text += "\n### Water Quality\n"
    for key, value in water_quality_data.items():
        output_text += f"- **{key}:** {value}\n"

    output_text += "\n### Traffic\n"
    for key, value in traffic_data.items():
        output_text += f"- **{key}:** {value}\n"

    output_text += "\n### Energy Usage\n"
    for key, value in energy_usage_data.items():
        output_text += f"- **{key}:** {value}\n"

    return output_text

def citizen_feedback_system(feedback):
    """Summarizes citizen feedback and provides basic routing information."""
    if not feedback:
        return "Please enter your feedback."

    # Summarize feedback using the IBM Granite model
    try:
        # Ensure tokenizer and model are accessible
        global tokenizer, model
        if 'tokenizer' not in globals() or 'model' not in globals():
             return "Error: Language model not loaded. Please run the model loading cell."

        prompt = f"Summarize the following feedback:\n{feedback}"
        inputs = tokenizer(prompt, return_tensors="pt")
        # Move inputs to GPU if available
        if torch.cuda.is_available():
            inputs = {k: v.to("cuda") for k, v in inputs.items()}
            model.to("cuda")

        # Generate summary - Adjust max_new_tokens for desired summary length and response time
        outputs = model.generate(**inputs, max_new_tokens=100)
        summarized_feedback = tokenizer.decode(outputs[0], skip_special_tokens=True)

        # Move model and inputs back to CPU if needed for other operations or to free GPU memory
        if torch.cuda.is_available():
             model.to("cpu")
             del inputs # Free up GPU memory

    except Exception as e:
        summarized_feedback = f"Error summarizing feedback: {e}"

    # Basic routing mechanism
    routing_info = "Based on your feedback, we suggest routing to the following department:\n\n"
    if "pothole" in feedback.lower() or "road" in feedback.lower():
        routing_info += "- Public Works Department."
    elif "park" in feedback.lower() or "green space" in feedback.lower():
        routing_info += "- Parks and Recreation Department."
    elif "water" in feedback.lower() or "sewage" in feedback.lower():
        routing_info += "- Water and Sewer Department."
    else:
        routing_info += "- General City Services."

    output_text = f"## Summarized Feedback:\n{summarized_feedback}\n\n"
    output_text += f"## Routing Information:\n{routing_info}"

    return output_text

def document_summarization_tool(uploaded_file):
    """Summarizes text from an uploaded PDF document."""
    if uploaded_file is None:
        return "Please upload a document."

    try:
        # Read the uploaded file in binary mode
        # Check if uploaded_file is bytes or a file path
        if isinstance(uploaded_file, str): # If it's a file path (common in some Gradio modes)
             with open(uploaded_file, 'rb') as f:
                  pdf_reader = PyPDF2.PdfReader(f)
        else: # Assume it's a file-like object or bytes
             pdf_reader = PyPDF2.PdfReader(io.BytesIO(uploaded_file))

        text = ""
        for page_num in range(len(pdf_reader.pages)):
            text += pdf_reader.pages[page_num].extract_text()

        if not text:
            return "Could not extract text from the document. Please try a different file or a document with extractable text."

        # Summarize text using the IBM Granite model
        # Ensure tokenizer and model are accessible
        global tokenizer, model
        if 'tokenizer' not in globals() or 'model' not in globals():
             return "Error: Language model not loaded. Please run the model loading cell."

        max_model_input_length = tokenizer.model_max_length if hasattr(tokenizer, 'model_max_length') and tokenizer.model_max_length is not None else 1024
        if len(text) > max_model_input_length:
            text = text[:max_model_input_length]
            warning_message = f"\n\n*Warning: Document truncated to {max_model_input_length} characters for summarization.*"
        else:
            warning_message = ""

        prompt = f"Summarize the following document:\n{text}"
        inputs = tokenizer(prompt, return_tensors="pt", max_length=max_model_input_length, truncation=True)
        # Move inputs to GPU if available
        if torch.cuda.is_available():
            inputs = {k: v.to("cuda") for k, v in inputs.items()}
            model.to("cuda")

        # Generate summary - Adjust max_new_tokens for desired summary length and response time
        outputs = model.generate(**inputs, max_new_tokens=200)
        summarized_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

        # Move model and inputs back to CPU if needed
        if torch.cuda.is_available():
             model.to("cpu")
             del inputs

        # Format the output
        output_markdown = f"## Original Text Snippet:\n"
        output_markdown += f"```\n{text[:500]}...\n```\n\n" # Display a shorter snippet
        output_markdown += f"## Summarized Text:\n"
        output_markdown += summarized_text
        output_markdown += warning_message

        return output_markdown

    except Exception as e:
        return f"Error processing document: {e}\nPlease ensure the uploaded file is a valid PDF."

def eco_advice_assistant(user_query):
    """Generates eco-friendly tips based on user queries."""
    if not user_query:
        return "Please enter a query to get eco-tips."

    try:
        # Generate eco-tips using the IBM Granite model
        # Ensure tokenizer and model are accessible
        global tokenizer, model
        if 'tokenizer' not in globals() or 'model' not in globals():
             return "Error: Language model not loaded. Please run the model loading cell."

        prompt = f"Provide eco-friendly tips based on the following query:\n{user_query}"
        inputs = tokenizer(prompt, return_tensors="pt")
        # Move inputs to GPU if available
        if torch.cuda.is_available():
            inputs = {k: v.to("cuda") for k, v in inputs.items()}
            model.to("cuda")

        # Generate eco-tips - Adjust max_new_tokens for desired response length and response time
        outputs = model.generate(**inputs, max_new_tokens=300) # Increased tokens for more comprehensive tips
        eco_tips = tokenizer.decode(outputs[0], skip_special_tokens=True)

        # Move model and inputs back to CPU if needed
        if torch.cuda.is_available():
             model.to("cpu")
             del inputs

        # Basic keyword-based suggestions
        suggestions = ""
        if "home" in user_query.lower():
            suggestions += "- Consider switching to energy-efficient light bulbs.\n"
            suggestions += "- Look into composting food waste.\n"
        if "office" in user_query.lower():
            suggestions += "- Encourage recycling programs.\n"
            suggestions += "- Suggest reducing paper usage.\n"
        if "local initiatives" in user_query.lower() or "community" in user_query.lower():
             suggestions += "- Check your local government website for community clean-up events.\n"
             suggestions += "- Look for local farmers markets for sustainable food options.\n"

        output_markdown = f"## Eco-Tips:\n{eco_tips}\n\n"
        if suggestions:
            output_markdown += f"## Further Suggestions:\n{suggestions}"

        return output_markdown

    except Exception as e:
        return f"Error generating eco-tips: {e}"


# Create Gradio Interfaces for each functionality with improved UI
health_interface = gr.Interface(
    fn=city_health_dashboard,
    inputs=[
        gr.Textbox(label="State", placeholder="e.g., California"),
        gr.Textbox(label="Country", placeholder="e.g., USA"),
        gr.Textbox(label="Place", placeholder="e.g., San Francisco")
    ],
    outputs="markdown",
    title="City Health Dashboard",
    description="Explore key environmental and urban metrics for your specified location. (Using placeholder data)"
)

feedback_interface = gr.Interface(
    fn=citizen_feedback_system,
    inputs=gr.Textbox(label="Enter your feedback", placeholder="Describe the issue or suggestion you have for the city."),
    outputs="markdown",
    title="Citizen Feedback System",
    description="Share your feedback about city services and infrastructure. The AI will summarize and suggest routing."
)

summarization_interface = gr.Interface(
    fn=document_summarization_tool,
    inputs=gr.File(label="Upload Document (PDF)", file_count="single"),
    outputs="markdown",
    title="Document Summarization Tool",
    description="Upload a PDF document to get a concise summary using the IBM Granite model."
)

eco_interface = gr.Interface(
    fn=eco_advice_assistant,
    inputs=gr.Textbox(label="Ask for eco-tips", placeholder="e.g., Eco-friendly tips for my kitchen?"),
    outputs="markdown",
    title="Eco-Advice Assistant",
    description="Get personalized eco-friendly tips for various aspects of your life using the IBM Granite model."
)

# Combine interfaces into a TabbedInterface with a theme
app = gr.TabbedInterface(
    [health_interface, feedback_interface, summarization_interface, eco_interface],
    ["City Health Dashboard", "Citizen Feedback System", "Document Summarization Tool", "Eco-Advice Assistant"],
    title="SUSTAINABLE SMART CITY ASSISTANT",
    theme=gr.themes.Soft() # Apply a theme
)

# Launch the Gradio application
app.launch(inline=True, share=True) # Enable sharing for Colab

"""## Implement the citizen feedback system

### Subtask:
Implement the citizen feedback system, including input fields for feedback, summarization using the IBM Granite model, and basic feedback routing.

**Reasoning**:
Implement the citizen feedback system functionality as described in the instructions, including adding a text input for feedback, using the loaded model to summarize, implementing basic routing based on keywords, and formatting the output.
"""

import gradio as gr

def citizen_feedback_system(feedback):
    """Summarizes citizen feedback and provides basic routing information."""
    if not feedback:
        return "Please enter your feedback."

    # Summarize feedback using the IBM Granite model
    try:
        prompt = f"Summarize the following feedback:\n{feedback}"
        inputs = tokenizer(prompt, return_tensors="pt")
        outputs = model.generate(**inputs, max_new_tokens=100)
        summarized_feedback = tokenizer.decode(outputs[0], skip_special_tokens=True)
    except Exception as e:
        summarized_feedback = f"Error summarizing feedback: {e}"

    # Basic routing mechanism
    routing_info = "Based on your feedback, we suggest routing to the following department:\n\n"
    if "pothole" in feedback.lower() or "road" in feedback.lower():
        routing_info += "- Public Works Department."
    elif "park" in feedback.lower() or "green space" in feedback.lower():
        routing_info += "- Parks and Recreation Department."
    elif "water" in feedback.lower() or "sewage" in feedback.lower():
        routing_info += "- Water and Sewer Department."
    else:
        routing_info += "- General City Services."

    output_text = f"## Summarized Feedback:\n{summarized_feedback}\n\n"
    output_text += f"## Routing Information:\n{routing_info}"

    return output_text

# Update the feedback_interface to use the new function and output markdown
feedback_interface = gr.Interface(
    fn=citizen_feedback_system,
    inputs=gr.Textbox(label="Enter your feedback"),
    outputs="markdown",
    title="Citizen Feedback System"
)

# Assuming other interfaces (health_interface, summarization_interface, eco_interface) are already defined
# If not, you would need to include their definitions here or ensure they are in the notebook's scope.

# Combine interfaces into a TabbedInterface (re-create the app with the updated interface)
# Need to redefine the other placeholder functions if they are not in the current cell
def city_health_dashboard(state, country, place):
    """Provides placeholder data for City Health Dashboard based on location."""
    # Placeholder data
    air_quality_data = {"PM2.5": "15 µg/m³", "AQI": "55 (Moderate)"}
    noise_levels_data = {"Average dB": "60 dB", "Peak dB": "75 dB"}
    water_quality_data = {"pH": "7.2", "Turbidity": "5 NTU"}
    traffic_data = {"Current Speed": "25 mph", "Congestion Level": "Medium"}
    energy_usage_data = {"Daily Consumption": "15 kWh/household", "Renewable Share": "20%"}

    output_text = f"## City Health Data for {place}, {state}, {country}\n\n"

    output_text += "### Air Quality\n"
    for key, value in air_quality_data.items():
        output_text += f"- **{key}:** {value}\n"

    output_text += "\n### Noise Levels\n"
    for key, value in noise_levels_data.items():
        output_text += f"- **{key}:** {value}\n"

    output_text += "\n### Water Quality\n"
    for key, value in water_quality_data.items():
        output_text += f"- **{key}:** {value}\n"

    output_text += "\n### Traffic\n"
    for key, value in traffic_data.items():
        output_text += f"- **{key}:** {value}\n"

    output_text += "\n### Energy Usage\n"
    for key, value in energy_usage_data.items():
        output_text += f"- **{key}:** {value}\n"

    return output_text

def document_summarization_tool(document):
    """Placeholder function for Document Summarization Tool."""
    if document is None:
        return "Please upload a document."
    # In a real implementation, you would add PDF parsing here
    return "Document uploaded. Summarization coming soon!"

def eco_advice_assistant(query):
    """Placeholder function for Eco-Advice Assistant."""
    return f"Eco-advice for '{query}' coming soon!"

health_interface = gr.Interface(
    fn=city_health_dashboard,
    inputs=[gr.Textbox(label="State"), gr.Textbox(label="Country"), gr.Textbox(label="Place")],
    outputs="markdown",  # Change output type to markdown for better formatting
    title="City Health Dashboard"
)

summarization_interface = gr.Interface(
    fn=document_summarization_tool,
    inputs=gr.File(label="Upload Document (PDF)"),
    outputs="text",
    title="Document Summarization Tool"
)

eco_interface = gr.Interface(
    fn=eco_advice_assistant,
    inputs=gr.Textbox(label="Ask for eco-tips"),
    outputs="text",
    title="Eco-Advice Assistant"
)


app = gr.TabbedInterface(
    [health_interface, feedback_interface, summarization_interface, eco_interface],
    ["City Health Dashboard", "Citizen Feedback System", "Document Summarization Tool", "Eco-Advice Assistant"],
    title="SUSTAINABLE SMART CITY ASSISTANT"
)

# Launch the Gradio application
app.launch(inline=True)

"""## Implement the document summarization tool

### Subtask:
Implement the document summarization tool, including a file uploader, text extraction from uploaded documents (focusing on PDF as an example), summarization using the IBM Granite model, and displaying the summarized text.

**Reasoning**:
Implement the document summarization tool by defining the function, adding PDF text extraction using PyPDF2, summarizing the text with the loaded model, formatting the output, and updating the Gradio interface.
"""

import gradio as gr
import PyPDF2
import io # Import io for file handling

def document_summarization_tool(uploaded_file):
    """Summarizes text from an uploaded PDF document."""
    if uploaded_file is None:
        return "Please upload a document."

    try:
        # Read the uploaded file in binary mode
        pdf_reader = PyPDF2.PdfReader(io.BytesIO(uploaded_file.encode('latin-1'))) # Encode as latin-1 to handle potential encoding issues
        text = ""
        for page_num in range(len(pdf_reader.pages)):
            text += pdf_reader.pages[page_num].extract_text()

        if not text:
            return "Could not extract text from the document. Please try a different file."

        # Summarize text using the IBM Granite model
        # Note: This is a basic summarization and might not handle very large documents well.
        # Truncate text if it's too long for the model's context window
        max_model_input_length = tokenizer.model_max_length if hasattr(tokenizer, 'model_max_length') and tokenizer.model_max_length is not None else 1024
        if len(text) > max_model_input_length:
            text = text[:max_model_input_length]
            warning_message = f"\n\n*Warning: Document truncated to {max_model_input_length} characters for summarization.*"
        else:
            warning_message = ""


        prompt = f"Summarize the following document:\n{text}"
        inputs = tokenizer(prompt, return_tensors="pt", max_length=max_model_input_length, truncation=True)
        outputs = model.generate(**inputs, max_new_tokens=200)
        summarized_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

        # Format the output
        output_markdown = f"## Original Text Snippet:\n"
        output_markdown += f"```\n{text[:1000]}...\n```\n\n" # Display a snippet
        output_markdown += f"## Summarized Text:\n"
        output_markdown += summarized_text
        output_markdown += warning_message


        return output_markdown

    except Exception as e:
        return f"Error processing document: {e}\nPlease ensure the uploaded file is a valid PDF."

# Update the summarization_interface to use the new function and output markdown
summarization_interface = gr.Interface(
    fn=document_summarization_tool,
    inputs=gr.File(label="Upload Document (PDF)"),
    outputs="markdown",  # Change output type to markdown
    title="Document Summarization Tool"
)

# Assuming other interfaces (health_interface, feedback_interface, eco_interface) are already defined
# If not, you would need to include their definitions here or ensure they are in the notebook's scope.

# Need to redefine the other placeholder functions if they are not in the current cell
def city_health_dashboard(state, country, place):
    """Provides placeholder data for City Health Dashboard based on location."""
    # Placeholder data
    air_quality_data = {"PM2.5": "15 µg/m³", "AQI": "55 (Moderate)"}
    noise_levels_data = {"Average dB": "60 dB", "Peak dB": "75 dB"}
    water_quality_data = {"pH": "7.2", "Turbidity": "5 NTU"}
    traffic_data = {"Current Speed": "25 mph", "Congestion Level": "Medium"}
    energy_usage_data = {"Daily Consumption": "15 kWh/household", "Renewable Share": "20%"}

    output_text = f"## City Health Data for {place}, {state}, {country}\n\n"

    output_text += "### Air Quality\n"
    for key, value in air_quality_data.items():
        output_text += f"- **{key}:** {value}\n"

    output_text += "\n### Noise Levels\n"
    for key, value in noise_levels_data.items():
        output_text += f"- **{key}:** {value}\n"

    output_text += "\n### Water Quality\n"
    for key, value in water_quality_data.items():
        output_text += f"- **{key}:** {value}\n"

    output_text += "\n### Traffic\n"
    for key, value in traffic_data.items():
        output_text += f"- **{key}:** {value}\n"

    output_text += "\n### Energy Usage\n"
    for key, value in energy_usage_data.items():
        output_text += f"- **{key}:** {value}\n"

    return output_text

def citizen_feedback_system(feedback):
    """Summarizes citizen feedback and provides basic routing information."""
    if not feedback:
        return "Please enter your feedback."

    # Summarize feedback using the IBM Granite model
    try:
        prompt = f"Summarize the following feedback:\n{feedback}"
        inputs = tokenizer(prompt, return_tensors="pt")
        outputs = model.generate(**inputs, max_new_tokens=100)
        summarized_feedback = tokenizer.decode(outputs[0], skip_special_tokens=True)
    except Exception as e:
        summarized_feedback = f"Error summarizing feedback: {e}"

    # Basic routing mechanism
    routing_info = "Based on your feedback, we suggest routing to the following department:\n\n"
    if "pothole" in feedback.lower() or "road" in feedback.lower():
        routing_info += "- Public Works Department."
    elif "park" in feedback.lower() or "green space" in feedback.lower():
        routing_info += "- Parks and Recreation Department."
    elif "water" in feedback.lower() or "sewage" in feedback.lower():
        routing_info += "- Water and Sewer Department."
    else:
        routing_info += "- General City Services."

    output_text = f"## Summarized Feedback:\n{summarized_feedback}\n\n"
    output_text += f"## Routing Information:\n{routing_info}"

    return output_text


def eco_advice_assistant(query):
    """Placeholder function for Eco-Advice Assistant."""
    return f"Eco-advice for '{query}' coming soon!"

health_interface = gr.Interface(
    fn=city_health_dashboard,
    inputs=[gr.Textbox(label="State"), gr.Textbox(label="Country"), gr.Textbox(label="Place")],
    outputs="markdown",  # Change output type to markdown for better formatting
    title="City Health Dashboard"
)

feedback_interface = gr.Interface(
    fn=citizen_feedback_system,
    inputs=gr.Textbox(label="Enter your feedback"),
    outputs="markdown",
    title="Citizen Feedback System"
)


eco_interface = gr.Interface(
    fn=eco_advice_assistant,
    inputs=gr.Textbox(label="Ask for eco-tips"),
    outputs="text",
    title="Eco-Advice Assistant"
)


app = gr.TabbedInterface(
    [health_interface, feedback_interface, summarization_interface, eco_interface],
    ["City Health Dashboard", "Citizen Feedback System", "Document Summarization Tool", "Eco-Advice Assistant"],
    title="SUSTAINABLE SMART CITY ASSISTANT"
)

# Launch the Gradio application
app.launch(inline=True)

"""## Implement the eco-advice assistant

### Subtask:
Implement the eco-advice assistant, including input fields for user queries, generating eco-tips using the IBM Granite model, and potentially incorporating suggestions for green alternatives or local initiatives.

**Reasoning**:
Implement the eco-advice assistant function, integrate the IBM Granite model for generating tips, and update the Gradio interface to use this function and display the results in markdown.
"""

import gradio as gr
import PyPDF2
import io

def city_health_dashboard(state, country, place):
    """Provides placeholder data for City Health Dashboard based on location."""
    # Placeholder data
    air_quality_data = {"PM2.5": "15 µg/m³", "AQI": "55 (Moderate)"}
    noise_levels_data = {"Average dB": "60 dB", "Peak dB": "75 dB"}
    water_quality_data = {"pH": "7.2", "Turbidity": "5 NTU"}
    traffic_data = {"Current Speed": "25 mph", "Congestion Level": "Medium"}
    energy_usage_data = {"Daily Consumption": "15 kWh/household", "Renewable Share": "20%"}

    output_text = f"## City Health Data for {place}, {state}, {country}\n\n"

    output_text += "### Air Quality\n"
    for key, value in air_quality_data.items():
        output_text += f"- **{key}:** {value}\n"

    output_text += "\n### Noise Levels\n"
    for key, value in noise_levels_data.items():
        output_text += f"- **{key}:** {value}\n"

    output_text += "\n### Water Quality\n"
    for key, value in water_quality_data.items():
        output_text += f"- **{key}:** {value}\n"

    output_text += "\n### Traffic\n"
    for key, value in traffic_data.items():
        output_text += f"- **{key}:** {value}\n"

    output_text += "\n### Energy Usage\n"
    for key, value in energy_usage_data.items():
        output_text += f"- **{key}:** {value}\n"

    return output_text

def citizen_feedback_system(feedback):
    """Summarizes citizen feedback and provides basic routing information."""
    if not feedback:
        return "Please enter your feedback."

    # Summarize feedback using the IBM Granite model
    try:
        prompt = f"Summarize the following feedback:\n{feedback}"
        inputs = tokenizer(prompt, return_tensors="pt")
        outputs = model.generate(**inputs, max_new_tokens=100)
        summarized_feedback = tokenizer.decode(outputs[0], skip_special_tokens=True)
    except Exception as e:
        summarized_feedback = f"Error summarizing feedback: {e}"

    # Basic routing mechanism
    routing_info = "Based on your feedback, we suggest routing to the following department:\n\n"
    if "pothole" in feedback.lower() or "road" in feedback.lower():
        routing_info += "- Public Works Department."
    elif "park" in feedback.lower() or "green space" in feedback.lower():
        routing_info += "- Parks and Recreation Department."
    elif "water" in feedback.lower() or "sewage" in feedback.lower():
        routing_info += "- Water and Sewer Department."
    else:
        routing_info += "- General City Services."

    output_text = f"## Summarized Feedback:\n{summarized_feedback}\n\n"
    output_text += f"## Routing Information:\n{routing_info}"

    return output_text

def document_summarization_tool(uploaded_file):
    """Summarizes text from an uploaded PDF document."""
    if uploaded_file is None:
        return "Please upload a document."

    try:
        pdf_reader = PyPDF2.PdfReader(io.BytesIO(uploaded_file.encode('latin-1')))
        text = ""
        for page_num in range(len(pdf_reader.pages)):
            text += pdf_reader.pages[page_num].extract_text()

        if not text:
            return "Could not extract text from the document. Please try a different file."

        max_model_input_length = tokenizer.model_max_length if hasattr(tokenizer, 'model_max_length') and tokenizer.model_max_length is not None else 1024
        if len(text) > max_model_input_length:
            text = text[:max_model_input_length]
            warning_message = f"\n\n*Warning: Document truncated to {max_model_input_length} characters for summarization.*"
        else:
            warning_message = ""

        prompt = f"Summarize the following document:\n{text}"
        inputs = tokenizer(prompt, return_tensors="pt", max_length=max_model_input_length, truncation=True)
        outputs = model.generate(**inputs, max_new_tokens=200)
        summarized_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

        output_markdown = f"## Original Text Snippet:\n"
        output_markdown += f"```\n{text[:1000]}...\n```\n\n"
        output_markdown += f"## Summarized Text:\n"
        output_markdown += summarized_text
        output_markdown += warning_message

        return output_markdown

    except Exception as e:
        return f"Error processing document: {e}\nPlease ensure the uploaded file is a valid PDF."

def eco_advice_assistant(user_query):
    """Generates eco-friendly tips based on user queries."""
    if not user_query:
        return "Please enter a query to get eco-tips."

    try:
        # Generate eco-tips using the IBM Granite model
        prompt = f"Provide eco-friendly tips based on the following query:\n{user_query}"
        inputs = tokenizer(prompt, return_tensors="pt")
        outputs = model.generate(**inputs, max_new_tokens=300) # Increased tokens for more comprehensive tips
        eco_tips = tokenizer.decode(outputs[0], skip_special_tokens=True)

        # Basic keyword-based suggestions
        suggestions = ""
        if "home" in user_query.lower():
            suggestions += "- Consider switching to energy-efficient light bulbs.\n"
            suggestions += "- Look into composting food waste.\n"
        if "office" in user_query.lower():
            suggestions += "- Encourage recycling programs.\n"
            suggestions += "- Suggest reducing paper usage.\n"
        if "local initiatives" in user_query.lower() or "community" in user_query.lower():
             suggestions += "- Check your local government website for community clean-up events.\n"
             suggestions += "- Look for local farmers markets for sustainable food options.\n"

        output_markdown = f"## Eco-Tips:\n{eco_tips}\n\n"
        if suggestions:
            output_markdown += f"## Further Suggestions:\n{suggestions}"

        return output_markdown

    except Exception as e:
        return f"Error generating eco-tips: {e}"

# Create Gradio Interfaces for each functionality
health_interface = gr.Interface(
    fn=city_health_dashboard,
    inputs=[gr.Textbox(label="State"), gr.Textbox(label="Country"), gr.Textbox(label="Place")],
    outputs="markdown",
    title="City Health Dashboard"
)

feedback_interface = gr.Interface(
    fn=citizen_feedback_system,
    inputs=gr.Textbox(label="Enter your feedback"),
    outputs="markdown",
    title="Citizen Feedback System"
)

summarization_interface = gr.Interface(
    fn=document_summarization_tool,
    inputs=gr.File(label="Upload Document (PDF)"),
    outputs="markdown",
    title="Document Summarization Tool"
)

eco_interface = gr.Interface(
    fn=eco_advice_assistant,
    inputs=gr.Textbox(label="Ask for eco-tips"),
    outputs="markdown",  # Change output type to markdown
    title="Eco-Advice Assistant"
)

# Combine interfaces into a TabbedInterface
app = gr.TabbedInterface(
    [health_interface, feedback_interface, summarization_interface, eco_interface],
    ["City Health Dashboard", "Citizen Feedback System", "Document Summarization Tool", "Eco-Advice Assistant"],
    title="SUSTAINABLE SMART CITY ASSISTANT"
)

# Launch the Gradio application
app.launch(inline=True)

"""## Integrate the ibm granite model

### Subtask:
Ensure the loaded model is used for summarization, feedback analysis, and eco-advice generation in the respective sections.

"""

# Reviewing the "Citizen Feedback System" code
print("Reviewing Citizen Feedback System code:")
print("""
def citizen_feedback_system(feedback):
    \"\"\"Summarizes citizen feedback and provides basic routing information.\"\"\"
    if not feedback:
        return "Please enter your feedback."

    # Summarize feedback using the IBM Granite model
    try:
        prompt = f"Summarize the following feedback:\\n{feedback}"
        inputs = tokenizer(prompt, return_tensors="pt")
        outputs = model.generate(**inputs, max_new_tokens=100)
        summarized_feedback = tokenizer.decode(outputs[0], skip_special_tokens=True)
    except Exception as e:
        summarized_feedback = f"Error summarizing feedback: {e}"

    # Basic routing mechanism
    routing_info = "Based on your feedback, we suggest routing to the following department:\\n\\n"
    if "pothole" in feedback.lower() or "road" in feedback.lower():
        routing_info += "- Public Works Department."
    elif "park" in feedback.lower() or "green space" in feedback.lower():
        routing_info += "- Parks and Recreation Department."
    elif "water" in feedback.lower() or "sewage" in feedback.lower():
        routing_info += "- Water and Sewer Department."
    else:
        routing_info += "- General City Services."

    output_text = f"## Summarized Feedback:\\n{summarized_feedback}\\n\\n"
    output_text += f"## Routing Information:\\n{routing_info}"

    return output_text
""")

# Reviewing the "Document Summarization Tool" code
print("\nReviewing Document Summarization Tool code:")
print("""
def document_summarization_tool(uploaded_file):
    \"\"\"Summarizes text from an uploaded PDF document.\"\"\"
    if uploaded_file is None:
        return "Please upload a document."

    try:
        pdf_reader = PyPDF2.PdfReader(io.BytesIO(uploaded_file.encode('latin-1')))
        text = ""
        for page_num in range(len(pdf_reader.pages)):
            text += pdf_reader.pages[page_num].extract_text()

        if not text:
            return "Could not extract text from the document. Please try a different file."

        max_model_input_length = tokenizer.model_max_length if hasattr(tokenizer, 'model_max_length') and tokenizer.model_max_length is not None else 1024
        if len(text) > max_model_input_length:
            text = text[:max_model_input_length]
            warning_message = f"\\n\\n*Warning: Document truncated to {max_model_input_length} characters for summarization.*"
        else:
            warning_message = ""

        prompt = f"Summarize the following document:\\n{text}"
        inputs = tokenizer(prompt, return_tensors="pt", max_length=max_model_input_length, truncation=True)
        outputs = model.generate(**inputs, max_new_tokens=200)
        summarized_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

        output_markdown = f"## Original Text Snippet:\\n"
        output_markdown += f"```\\n{text[:1000]}...\\n```\\n\\n"
        output_markdown += f"## Summarized Text:\\n"
        output_markdown += summarized_text
        output_markdown += warning_message

        return output_markdown

    except Exception as e:
        return f"Error processing document: {e}\\nPlease ensure the uploaded file is a valid PDF."
""")

# Reviewing the "Eco-Advice Assistant" code
print("\nReviewing Eco-Advice Assistant code:")
print("""
def eco_advice_assistant(user_query):
    \"\"\"Generates eco-friendly tips based on user queries.\"\"\"
    if not user_query:
        return "Please enter a query to get eco-tips."

    try:
        # Generate eco-tips using the IBM Granite model
        prompt = f"Provide eco-friendly tips based on the following query:\\n{user_query}"
        inputs = tokenizer(prompt, return_tensors="pt")
        outputs = model.generate(**inputs, max_new_tokens=300) # Increased tokens for more comprehensive tips
        eco_tips = tokenizer.decode(outputs[0], skip_special_tokens=True)

        # Basic keyword-based suggestions
        suggestions = ""
        if "home" in user_query.lower():
            suggestions += "- Consider switching to energy-efficient light bulbs.\\n"
            suggestions += "- Look into composting food waste.\\n"
        if "office" in user_query.lower():
            suggestions += "- Encourage recycling programs.\\n"
            suggestions += "- Suggest reducing paper usage.\\n"
        if "local initiatives" in user_query.lower() or "community" in user_query.lower():
             suggestions += "- Check your local government website for community clean-up events.\\n"
             suggestions += "- Look for local farmers markets for sustainable food options.\\n"

        output_markdown = f"## Eco-Tips:\\n{eco_tips}\\n\\n"
        if suggestions:
            output_markdown += f"## Further Suggestions:\\n{suggestions}"

        return output_markdown

    except Exception as e:
        return f"Error generating eco-tips: {e}"
""")

"""## Refine and improve

### Subtask:
Add styling, error handling, and improve the user interface of the Gradio application.

**Reasoning**:
Implement the styling, error handling, and UI improvements for the Gradio application, including adding descriptive labels and help text, enhancing output presentation with markdown, implementing error handling in the functions, adding loading indicators implicitly handled by Gradio, and exploring Gradio's theming options.
"""

import gradio as gr
import PyPDF2
import io # Import io for file handling
# tokenizer and model are assumed to be loaded from a previous cell

def city_health_dashboard(state, country, place):
    """Provides placeholder data for City Health Dashboard based on location."""
    if not state and not country and not place:
        return "Please enter a location (State, Country, or Place) to view the dashboard."

    # Placeholder data
    air_quality_data = {"PM2.5": "15 µg/m³", "AQI": "55 (Moderate)"}
    noise_levels_data = {"Average dB": "60 dB", "Peak dB": "75 dB"}
    water_quality_data = {"pH": "7.2", "Turbidity": "5 NTU"}
    traffic_data = {"Current Speed": "25 mph", "Congestion Level": "Medium"}
    energy_usage_data = {"Daily Consumption": "15 kWh/household", "Renewable Share": "20%"}

    output_text = f"## City Health Data for {place}, {state}, {country}\n\n"

    output_text += "### Air Quality\n"
    for key, value in air_quality_data.items():
        output_text += f"- **{key}:** {value}\n"

    output_text += "\n### Noise Levels\n"
    for key, value in noise_levels_data.items():
        output_text += f"- **{key}:** {value}\n"

    output_text += "\n### Water Quality\n"
    for key, value in water_quality_data.items():
        output_text += f"- **{key}:** {value}\n"

    output_text += "\n### Traffic\n"
    for key, value in traffic_data.items():
        output_text += f"- **{key}:** {value}\n"

    output_text += "\n### Energy Usage\n"
    for key, value in energy_usage_data.items():
        output_text += f"- **{key}:** {value}\n"

    return output_text

def citizen_feedback_system(feedback):
    """Summarizes citizen feedback and provides basic routing information."""
    if not feedback:
        return "Please enter your feedback."

    # Summarize feedback using the IBM Granite model
    try:
        prompt = f"Summarize the following feedback:\n{feedback}"
        inputs = tokenizer(prompt, return_tensors="pt")
        outputs = model.generate(**inputs, max_new_tokens=100)
        summarized_feedback = tokenizer.decode(outputs[0], skip_special_tokens=True)
    except Exception as e:
        summarized_feedback = f"Error summarizing feedback: {e}"

    # Basic routing mechanism
    routing_info = "Based on your feedback, we suggest routing to the following department:\n\n"
    if "pothole" in feedback.lower() or "road" in feedback.lower():
        routing_info += "- Public Works Department."
    elif "park" in feedback.lower() or "green space" in feedback.lower():
        routing_info += "- Parks and Recreation Department."
    elif "water" in feedback.lower() or "sewage" in feedback.lower():
        routing_info += "- Water and Sewer Department."
    else:
        routing_info += "- General City Services."

    output_text = f"## Summarized Feedback:\n{summarized_feedback}\n\n"
    output_text += f"## Routing Information:\n{routing_info}"

    return output_text

def document_summarization_tool(uploaded_file):
    """Summarizes text from an uploaded PDF document."""
    if uploaded_file is None:
        return "Please upload a document."

    try:
        # Read the uploaded file in binary mode
        pdf_reader = PyPDF2.PdfReader(io.BytesIO(uploaded_file.encode('latin-1'))) # Encode as latin-1 to handle potential encoding issues
        text = ""
        for page_num in range(len(pdf_reader.pages)):
            text += pdf_reader.pages[page_num].extract_text()

        if not text:
            return "Could not extract text from the document. Please try a different file or a document with extractable text."

        # Summarize text using the IBM Granite model
        # Note: This is a basic summarization and might not handle very large documents well.
        # Truncate text if it's too long for the model's context window
        max_model_input_length = tokenizer.model_max_length if hasattr(tokenizer, 'model_max_length') and tokenizer.model_max_length is not None else 1024
        if len(text) > max_model_input_length:
            text = text[:max_model_input_length]
            warning_message = f"\n\n*Warning: Document truncated to {max_model_input_length} characters for summarization.*"
        else:
            warning_message = ""


        prompt = f"Summarize the following document:\n{text}"
        inputs = tokenizer(prompt, return_tensors="pt", max_length=max_model_input_length, truncation=True)
        outputs = model.generate(**inputs, max_new_tokens=200)
        summarized_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

        # Format the output
        output_markdown = f"## Original Text Snippet:\n"
        output_markdown += f"```\n{text[:1000]}...\n```\n\n" # Display a snippet
        output_markdown += f"## Summarized Text:\n"
        output_markdown += summarized_text
        output_markdown += warning_message


        return output_markdown

    except Exception as e:
        return f"Error processing document: {e}\nPlease ensure the uploaded file is a valid PDF."

def eco_advice_assistant(user_query):
    """Generates eco-friendly tips based on user queries."""
    if not user_query:
        return "Please enter a query to get eco-tips."

    try:
        # Generate eco-tips using the IBM Granite model
        prompt = f"Provide eco-friendly tips based on the following query:\n{user_query}"
        inputs = tokenizer(prompt, return_tensors="pt")
        outputs = model.generate(**inputs, max_new_tokens=300) # Increased tokens for more comprehensive tips
        eco_tips = tokenizer.decode(outputs[0], skip_special_tokens=True)

        # Basic keyword-based suggestions
        suggestions = ""
        if "home" in user_query.lower():
            suggestions += "- Consider switching to energy-efficient light bulbs.\n"
            suggestions += "- Look into composting food waste.\n"
        if "office" in user_query.lower():
            suggestions += "- Encourage recycling programs.\n"
            suggestions += "- Suggest reducing paper usage.\n"
        if "local initiatives" in user_query.lower() or "community" in user_query.lower():
             suggestions += "- Check your local government website for community clean-up events.\n"
             suggestions += "- Look for local farmers markets for sustainable food options.\n"

        output_markdown = f"## Eco-Tips:\n{eco_tips}\n\n"
        if suggestions:
            output_markdown += f"## Further Suggestions:\n{suggestions}"

        return output_markdown

    except Exception as e:
        return f"Error generating eco-tips: {e}"


# Create Gradio Interfaces for each functionality with improved UI
health_interface = gr.Interface(
    fn=city_health_dashboard,
    inputs=[
        gr.Textbox(label="State", placeholder="e.g., California"),
        gr.Textbox(label="Country", placeholder="e.g., USA"),
        gr.Textbox(label="Place", placeholder="e.g., San Francisco")
    ],
    outputs="markdown",
    title="City Health Dashboard",
    description="Explore key environmental and urban metrics for your specified location. (Using placeholder data)"
)

feedback_interface = gr.Interface(
    fn=citizen_feedback_system,
    inputs=gr.Textbox(label="Enter your feedback", placeholder="Describe the issue or suggestion you have for the city."),
    outputs="markdown",
    title="Citizen Feedback System",
    description="Share your feedback about city services and infrastructure. The AI will summarize and suggest routing."
)

summarization_interface = gr.Interface(
    fn=document_summarization_tool,
    inputs=gr.File(label="Upload Document (PDF)", file_count="single"),
    outputs="markdown",
    title="Document Summarization Tool",
    description="Upload a PDF document to get a concise summary using the IBM Granite model."
)

eco_interface = gr.Interface(
    fn=eco_advice_assistant,
    inputs=gr.Textbox(label="Ask for eco-tips", placeholder="e.g., Eco-friendly tips for my kitchen?"),
    outputs="markdown",
    title="Eco-Advice Assistant",
    description="Get personalized eco-friendly tips for various aspects of your life using the IBM Granite model."
)

# Combine interfaces into a TabbedInterface with a theme
app = gr.TabbedInterface(
    [health_interface, feedback_interface, summarization_interface, eco_interface],
    ["City Health Dashboard", "Citizen Feedback System", "Document Summarization Tool", "Eco-Advice Assistant"],
    title="SUSTAINABLE SMART CITY ASSISTANT",
    theme=gr.themes.Soft() # Apply a theme
)

# Launch the Gradio application
app.launch(inline=True, share=True) # Enable sharing for Colab

"""## Deploy with gradio on colab

### Subtask:
Set up the necessary code to run the Gradio application within Google Colab.

**Reasoning**:
Set up the necessary code to run the Gradio application within Google Colab by creating the app.py file, running it as a subprocess, and creating a tunnel to access it.
"""

import google.colab.output
import subprocess
import gradio as gr
import PyPDF2
import io
import torch # Import torch for GPU handling and float16

# tokenizer and model are assumed to be loaded from a previous cell

def city_health_dashboard(state, country, place):
    """Provides placeholder data for City Health Dashboard based on location."""
    if not state and not country and not place:
        return "Please enter a location (State, Country, or Place) to view the dashboard."

    # Placeholder data
    air_quality_data = {"PM2.5": "15 µg/m³", "AQI": "55 (Moderate)"}
    noise_levels_data = {"Average dB": "60 dB", "Peak dB": "75 dB"}
    water_quality_data = {"pH": "7.2", "Turbidity": "5 NTU"}
    traffic_data = {"Current Speed": "25 mph", "Congestion Level": "Medium"}
    energy_usage_data = {"Daily Consumption": "15 kWh/household", "Renewable Share": "20%"}

    output_text = f"## City Health Data for {place}, {state}, {country}\n\n"

    output_text += "### Air Quality\n"
    for key, value in air_quality_data.items():
        output_text += f"- **{key}:** {value}\n"

    output_text += "\n### Noise Levels\n"
    for key, value in noise_levels_data.items():
        output_text += f"- **{key}:** {value}\n"

    output_text += "\n### Water Quality\n"
    for key, value in water_quality_data.items():
        output_text += f"- **{key}:** {value}\n"

    output_text += "\n### Traffic\n"
    for key, value in traffic_data.items():
        output_text += f"- **{key}:** {value}\n"

    output_text += "\n### Energy Usage\n"
    for key, value in energy_usage_data.items():
        output_text += f"- **{key}:** {value}\n"

    return output_text

def citizen_feedback_system(feedback):
    """Summarizes citizen feedback and provides basic routing information."""
    if not feedback:
        return "Please enter your feedback."

    # Summarize feedback using the IBM Granite model (Potential source of latency)
    try:
        # Ensure tokenizer and model are accessible
        global tokenizer, model
        if 'tokenizer' not in globals() or 'model' not in globals():
             return "Error: Language model not loaded. Please run the model loading cell."

        prompt = f"Summarize the following feedback:\n{feedback}"
        inputs = tokenizer(prompt, return_tensors="pt")

        # Move inputs and model to GPU if available and use float16
        if torch.cuda.is_available():
            inputs = {k: v.to("cuda") for k, v in inputs.items()}
            model.to("cuda")
            model.half() # Use float16 for mixed precision


        outputs = model.generate(**inputs, max_new_tokens=100)
        summarized_feedback = tokenizer.decode(outputs[0], skip_special_tokens=True)

        # Move model back to CPU if needed for other operations or to free GPU memory
        if torch.cuda.is_available():
             model.to("cpu")
             del inputs # Free up GPU memory

    except Exception as e:
        summarized_feedback = f"Error summarizing feedback: {e}"

    # Basic routing mechanism
    routing_info = "Based on your feedback, we suggest routing to the following department:\n\n"
    if "pothole" in feedback.lower() or "road" in feedback.lower():
        routing_info += "- Public Works Department."
    elif "park" in feedback.lower() or "green space" in feedback.lower():
        routing_info += "- Parks and Recreation Department."
    elif "water" in feedback.lower() or "sewage" in feedback.lower():
        routing_info += "- Water and Sewer Department."
    else:
        routing_info += "- General City Services."

    output_text = f"## Summarized Feedback:\n{summarized_feedback}\n\n"
    output_text += f"## Routing Information:\n{routing_info}"

    return output_text

def document_summarization_tool(uploaded_file):
    """Summarizes text from an uploaded PDF document."""
    if uploaded_file is None:
        return "Please upload a document."

    try:
        # Read the uploaded file in binary mode
        # Check if uploaded_file is bytes or a file path
        if isinstance(uploaded_file, str): # If it's a file path (common in some Gradio modes)
             with open(uploaded_file, 'rb') as f:
                  pdf_reader = PyPDF2.PdfReader(f)
        else: # Assume it's a file-like object or bytes
             pdf_reader = PyPDF2.PdfReader(io.BytesIO(uploaded_file))


        text = ""
        for page_num in range(len(pdf_reader.pages)):
            text += pdf_reader.pages[page_num].extract_text()

        if not text:
            return "Could not extract text from the document. Please try a different file or a document with extractable text."

        # Summarize text using the IBM Granite model
        # Note: This is a basic summarization and might not handle very large documents well.
        # Truncate text if it's too long for the model's context window
        max_model_input_length = tokenizer.model_max_length if hasattr(tokenizer, 'model_max_length') and tokenizer.model_max_length is not None else 1024
        if len(text) > max_model_input_length:
            text = text[:max_model_input_length]
            warning_message = f"\n\n*Warning: Document truncated to {max_model_input_length} characters for summarization.*"
        else:
            warning_message = ""


        prompt = f"Summarize the following document:\n{text}"
        inputs = tokenizer(prompt, return_tensors="pt", max_length=max_model_input_length, truncation=True)

        # Move inputs and model to GPU if available and use float16
        if torch.cuda.is_available():
            inputs = {k: v.to("cuda") for k, v in inputs.items()}
            model.to("cuda")
            model.half() # Use float16 for mixed precision

        outputs = model.generate(**inputs, max_new_tokens=200)
        summarized_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

        # Move model back to CPU if needed
        if torch.cuda.is_available():
             model.to("cpu")
             del inputs

        # Format the output
        output_markdown = f"## Original Text Snippet:\n"
        output_markdown += f"```\n{text[:1000]}...\n```\n\n" # Display a snippet
        output_markdown += f"## Summarized Text:\n"
        output_markdown += summarized_text
        output_markdown += warning_message


        return output_markdown

    except Exception as e:
        return f"Error processing document: {e}\nPlease ensure the uploaded file is a valid PDF."

def eco_advice_assistant(user_query):
    """Generates eco-friendly tips based on user queries."""
    if not user_query:
        return "Please enter a query to get eco-tips."

    try:
        # Generate eco-tips using the IBM Granite model (Potential source of latency)
        # Ensure tokenizer and model are accessible and on the correct device/dtype
        global tokenizer, model
        if 'tokenizer' not in globals() or 'model' not in globals():
             return "Error: Language model not loaded. Please run the model loading cell."

        prompt = f"Provide eco-friendly tips based on the following query:\n{user_query}"
        inputs = tokenizer(prompt, return_tensors="pt")

        # Move inputs and model to GPU if available and use float16
        if torch.cuda.is_available():
            inputs = {k: v.to("cuda") for k, v in inputs.items()}
            model.to("cuda")
            model.half() # Use float16 for mixed precision

        outputs = model.generate(**inputs, max_new_tokens=300) # Increased tokens for more comprehensive tips
        eco_tips = tokenizer.decode(outputs[0], skip_special_tokens=True)

        # Move model back to CPU if needed
        if torch.cuda.is_available():
             model.to("cpu")
             del inputs

        # Basic keyword-based suggestions
        suggestions = ""
        if "home" in user_query.lower():
            suggestions += "- Consider switching to energy-efficient light bulbs.\n"
            suggestions += "- Look into composting food waste.\n"
        if "office" in user_query.lower():
            suggestions += "- Encourage recycling programs.\n"
            suggestions += "- Suggest reducing paper usage.\n"
        if "local initiatives" in user_query.lower() or "community" in user_query.lower():
             suggestions += "- Check your local government website for community clean-up events.\n"
             suggestions += "- Look for local farmers markets for sustainable food options.\n"

        output_markdown = f"## Eco-Tips:\n{eco_tips}\n\n"
        if suggestions:
            output_markdown += f"## Further Suggestions:\n{suggestions}"

        return output_markdown

    except Exception as e:
        return f"Error generating eco-tips: {e}"


# Create Gradio Interfaces for each functionality with improved UI
health_interface = gr.Interface(
    fn=city_health_dashboard,
    inputs=[
        gr.Textbox(label="State", placeholder="e.g., California"),
        gr.Textbox(label="Country", placeholder="e.g., USA"),
        gr.Textbox(label="Place", placeholder="e.g., San Francisco")
    ],
    outputs="markdown",
    title="City Health Dashboard",
    description="Explore key environmental and urban metrics for your specified location. (Using placeholder data)"
)

feedback_interface = gr.Interface(
    fn=citizen_feedback_system,
    inputs=gr.Textbox(label="Enter your feedback", placeholder="Describe the issue or suggestion you have for the city."),
    outputs="markdown",
    title="Citizen Feedback System",
    description="Share your feedback about city services and infrastructure. The AI will summarize and suggest routing."
)

summarization_interface = gr.Interface(
    fn=document_summarization_tool,
    inputs=gr.File(label="Upload Document (PDF)", file_count="single"),
    outputs="markdown",
    title="Document Summarization Tool",
    description="Upload a PDF document to get a concise summary using the IBM Granite model."
)

eco_interface = gr.Interface(
    fn=eco_advice_assistant,
    inputs=gr.Textbox(label="Ask for eco-tips", placeholder="e.g., Eco-friendly tips for my kitchen?"),
    outputs="markdown",
    title="Eco-Advice Assistant",
    description="Get personalized eco-friendly tips for various aspects of your life using the IBM Granite model."
)

# Combine interfaces into a TabbedInterface with a theme
app = gr.TabbedInterface(
    [health_interface, feedback_interface, summarization_interface, eco_interface],
    ["City Health Dashboard", "Citizen Feedback System", "Document Summarization Tool", "Eco-Advice Assistant"],
    title="SUSTAINABLE SMART CITY ASSISTANT",
    theme=gr.themes.Soft() # Apply a theme
)

# Launch the Gradio application
app.launch(inline=True, share=True)